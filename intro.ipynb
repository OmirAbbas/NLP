{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "628314ec-fe3d-4d08-9cc9-07461e413e49",
   "metadata": {},
   "source": [
    "# Introduction to Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b6b3af-f9b2-41e6-8d6d-7bc96ffef54e",
   "metadata": {},
   "source": [
    "### What is Natural Language Processing (NLP)\n",
    "\n",
    "Natural Language Processing (NLP)\n",
    "- The engineering discipline of doing what people do with language, but using computers\n",
    "\n",
    "Computational Linguistics (CL)\n",
    "- The science of doing what linguists do with language, but using computers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70405008-6ba9-42b4-84af-90c43ee41ce7",
   "metadata": {},
   "source": [
    "\n",
    "Natural language processing (NLP) is the discipline of building machines that can manipulate human language — or data that resembles human language — in the way that it is written, spoken, and organized. \n",
    "\n",
    "It evolved from computational linguistics, which uses computer science to understand the principles of language, but rather than developing theoretical frameworks, NLP is an engineering discipline that seeks to build technology to accomplish useful tasks.\n",
    "\n",
    "NLP can be divided into two overlapping subfields:\n",
    "\n",
    "\n",
    "- natural language understanding (NLU), which focuses on semantic analysis or determining the intended meaning of text.\n",
    "\n",
    "- natural language generation (NLG), which focuses on text generation by a machine. \n",
    "\n",
    "NLP is separate from — but often used in conjunction with — speech recognition, which seeks to parse spoken language into words, turning sound into text and vice versa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e91b00-0fa4-4abb-b3a3-eb43213778ff",
   "metadata": {},
   "source": [
    "## What is Natural Language Processing (NLP) Used For?\n",
    "\n",
    "NLP is used for a wide variety of language-related tasks, including answering questions, classifying text in a variety of ways, and conversing with users. \n",
    "\n",
    "Here are 11 tasks that can be solved by NLP:\n",
    "\n",
    "- Sentiment analysis is the process of classifying the emotional intent of text. Generally, the input to a sentiment classification model is a piece of text, and the output is the probability that the sentiment expressed is positive, negative, or neutral. Typically, this probability is based on either hand-generated features, word n-grams, TF-IDF features, or using deep learning models to capture sequential long- and short-term dependencies. Sentiment analysis is used to classify customer reviews on various online platforms as well as for niche applications like identifying signs of mental illness in online comments.\n",
    "\n",
    "- Toxicity classification is a branch of sentiment analysis where the aim is not just to classify hostile intent but also to classify particular categories such as threats, insults, obscenities, and hatred towards certain identities. The input to such a model is text, and the output is generally the probability of each class of toxicity. Toxicity classification models can be used to moderate and improve online conversations by silencing offensive comments, detecting hate speech, or scanning documents for defamation. \n",
    "\n",
    "- Machine translation automates translation between different languages. The input to such a model is text in a specified source language, and the output is the text in a specified target language. Google Translate is perhaps the most famous mainstream application. Such models are used to improve communication between people on social-media platforms such as Facebook or Skype. Effective approaches to machine translation can distinguish between words with similar meanings. Some systems also perform language identification; that is, classifying text as being in one language or another.\n",
    "\n",
    "- Named entity recognition aims to extract entities in a piece of text into predefined categories such as personal names, organizations, locations, and quantities. The input to such a model is generally text, and the output is the various named entities along with their start and end positions. Named entity recognition is useful in applications such as summarizing news articles and combating disinformation.\n",
    "\n",
    "\n",
    "- Spam detection is a prevalent binary classification problem in NLP, where the purpose is to classify emails as either spam or not. Spam detectors take as input an email text along with various other subtexts like title and sender’s name. They aim to output the probability that the mail is spam. Email providers like Gmail use such models to provide a better user experience by detecting unsolicited and unwanted emails and moving them to a designated spam folder. \n",
    "\n",
    "- Grammatical error correction models encode grammatical rules to correct the grammar within text. This is viewed mainly as a sequence-to-sequence task, where a model is trained on an ungrammatical sentence as input and a correct sentence as output. Online grammar checkers like Grammarly and word-processing systems like Microsoft Word use such systems to provide a better writing experience to their customers. Schools also use them to grade student essays. \n",
    "\n",
    "- Topic modeling is an unsupervised text mining task that takes a corpus of documents and discovers abstract topics within that corpus. The input to a topic model is a collection of documents, and the output is a list of topics that defines words for each topic as well as assignment proportions of each topic in a document. Latent Dirichlet Allocation (LDA), one of the most popular topic modeling techniques, tries to view a document as a collection of topics and a topic as a collection of words. Topic modeling is being used commercially to help lawyers find evidence in legal documents. \n",
    "\n",
    "- Text generation, more formally known as natural language generation (NLG), produces text that’s similar to human-written text. Such models can be fine-tuned to produce text in different genres and formats — including tweets, blogs, and even computer code. Text generation has been performed using Markov processes, LSTMs, BERT, GPT-2, LaMDA, and other approaches. It’s particularly useful for autocomplete and chatbots.\n",
    "\n",
    "\n",
    "    - Autocomplete predicts what word comes next, and autocomplete systems of varying complexity are used in chat applications like WhatsApp. Google uses autocomplete to predict search queries. One of the most famous models for autocomplete is GPT-2, which has been used to write articles, song lyrics, and much more. \n",
    "    \n",
    "     - Chatbots automate one side of a conversation while a human conversant generally supplies the other side. They can be divided into the following two categories:\n",
    "\n",
    "        - Database query: We have a database of questions and answers, and we would like a user to query it using natural language. \n",
    "    \n",
    "        - Conversation generation: These chatbots can simulate dialogue with a human partner. Some are capable of engaging in wide-ranging conversations. A high-profile example is Google’s LaMDA, which provided such human-like answers to questions that one of its developers was convinced that it had feelings.\n",
    "\n",
    "\n",
    "- Information retrieval finds the documents that are most relevant to a query. This is a problem every search and recommendation system faces. The goal is not to answer a particular query but to retrieve, from a collection of documents that may be numbered in the millions, a set that is most relevant to the query. Document retrieval systems mainly execute two processes: indexing and matching. In most modern systems, indexing is done by a vector space model through Two-Tower Networks, while matching is done using similarity or distance scores. Google recently integrated its search function with a multimodal information retrieval model that works with text, image, and video data.\n",
    " \n",
    " \n",
    "- Summarization is the task of shortening text to highlight the most relevant information. Researchers at Salesforce developed a summarizer that also evaluates factual consistency to ensure that its output is accurate. Summarization is divided into two method classes:\n",
    "\n",
    "    - Extractive summarization focuses on extracting the most important sentences from a long text and combining these to form a summary. Typically, extractive summarization scores each sentence in an input text and then selects several sentences to form the summary.\n",
    "\n",
    "    - Abstractive summarization produces a summary by paraphrasing. This is similar to writing the abstract that includes words and sentences that are not present in the original text. Abstractive summarization is usually modeled as a sequence-to-sequence task, where the input is a long-form text and the output is a summary.\n",
    "\n",
    "\n",
    "- Question answering deals with answering questions posed by humans in a natural language. One of the most notable examples of question answering was Watson, which in 2011 played the television game-show Jeopardy against human champions and won by substantial margins. Generally, question-answering tasks come in two flavors:\n",
    "\n",
    "    - Multiple choice: The multiple-choice question problem is composed of a question and a set of possible answers. The learning task is to pick the correct answer. \n",
    "    - Open domain: In open-domain question answering, the model provides answers to questions in natural language without any options provided, often by querying a large number of texts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bc3960-782e-4326-9787-f784ab38efa5",
   "metadata": {},
   "source": [
    "### What does an NLP system need to “know”?\n",
    "- Language consists of many levels of structure\n",
    "\n",
    "- Humans fluently integrate all of these in producing and understanding language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c9be1b5-ff19-45ef-a20a-01922deb5363",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"langlevel.png\" width=\"500\" height=\"350\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(url=\"langlevel.png\", width=500, height=350)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa62861-babb-4bf2-84c1-9d2643f7df66",
   "metadata": {},
   "source": [
    "## Why is NLP hard?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf9532d-d334-4905-b82e-21f00e4c4bb9",
   "metadata": {},
   "source": [
    "- Ambiguity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983bb7a9-f93a-4014-82a8-f406b1734273",
   "metadata": {},
   "source": [
    "    - Word sense\n",
    "        • I went to the bank to deposit my check.\n",
    "        • I went to the bank to look out at the river"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cbd8fc-ac4a-4037-9dc7-307b38ffb32f",
   "metadata": {},
   "source": [
    "    - At the syntactic level\n",
    "        - I saw the man on the hill with the telescope"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7264f9be-8879-48ea-9c65-074fb2fe6db2",
   "metadata": {},
   "source": [
    "    - Structural ambiguity\n",
    "        - Time flies like an arrow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d150c5-3ddf-407c-82e1-f04f56ab34d7",
   "metadata": {},
   "source": [
    "NLP challenge: how can we model ambiguity, and choose the correct\n",
    "analysis in context? Approach: learn from data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1c612d-07f4-4f9e-8c29-11d9b97881dc",
   "metadata": {},
   "source": [
    "- Sparsity \n",
    "    - Even in a very large corpus, there will be a lot of infrequent words (implication of Zipf’s law)\n",
    "    - The same holds for many other levels of linguistic structure\n",
    "\n",
    "Core NLP challenge: we need to estimate probabilities or to be able to\n",
    "make predictions for things we have rarely or never seen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095ce9bd-0bac-46e6-a3ce-01a6cb094e3b",
   "metadata": {},
   "source": [
    "   - Variation and Expressivity\n",
    "        - The same meaning can be expressed with different forms\n",
    "            - I saw the man\n",
    "            - The man was seen by me\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966b9da6-f54e-4415-a69f-92374cfc4810",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
